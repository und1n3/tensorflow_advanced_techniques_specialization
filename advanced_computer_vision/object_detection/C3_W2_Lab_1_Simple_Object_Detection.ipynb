{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmANPR2jhCR6"
      },
      "source": [
        "# Simple Object Detection in Tensorflow\n",
        "\n",
        "This lab will walk you through how to use object detection models available in [Tensorflow Hub](https://www.tensorflow.org/hub). In the following sections, you will:\n",
        "\n",
        "* explore the Tensorflow Hub for object detection models\n",
        "* load the models in your workspace\n",
        "* preprocess an image for inference\n",
        "* run inference on the models and inspect the output\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DkMLuGDhCR6"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OEoRKdmByrb0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image\n",
        "from PIL import ImageOps\n",
        "import tempfile\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb8MBgTOhCR6"
      },
      "source": [
        "### Download the model from Tensorflow Hub\n",
        "\n",
        "Tensorflow Hub is a repository of trained machine learning models which you can reuse in your own projects.\n",
        "- You can see the domains covered [here](https://tfhub.dev/) and its subcategories.\n",
        "- For this lab, you will want to look at the [image object detection subcategory](https://tfhub.dev/s?module-type=image-object-detection).\n",
        "- You can select a model to see more information about it and copy the URL so you can download it to your workspace.\n",
        "- We selected a [inception resnet version 2](https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1)\n",
        "- You can also modify this following cell to choose the other model that we selected, [ssd mobilenet version 2](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C9pCzz4uy20U"
      },
      "outputs": [],
      "source": [
        "# you can switch the commented lines here to pick the other model\n",
        "\n",
        "# inception resnet version 2\n",
        "module_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"\n",
        "\n",
        "# You can choose ssd mobilenet version 2 instead and compare the results\n",
        "#module_handle = \"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3trj5FbhCR6"
      },
      "source": [
        "#### Load the model\n",
        "\n",
        "Next, you'll load the model specified by the `module_handle`.\n",
        "- This will take a few minutes to load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0WHkGDHfhCR6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "I0000 00:00:1738774386.566139    2655 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21770 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "model = hub.load(module_handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ey0FpHGhCR6"
      },
      "source": [
        "#### Choose the default signature\n",
        "\n",
        "Some models in the Tensorflow hub can be used for different tasks. So each model's documentation should show what *signature* to use when running the model.\n",
        "- If you want to see if a model has more than one signature then you can do something like `print(hub.load(module_handle).signatures.keys())`. In your case, the models you will be using only have the `default` signature so you don't have to worry about other types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X1BU7AGthCR6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KeysView(_SignatureMap({'default': <ConcreteFunction () -> Dict[['detection_scores', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)], ['detection_class_names', TensorSpec(shape=(None, 1), dtype=tf.string, name=None)], ['detection_class_entities', TensorSpec(shape=(None, 1), dtype=tf.string, name=None)], ['detection_boxes', TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)], ['detection_class_labels', TensorSpec(shape=(None, 1), dtype=tf.int64, name=None)]] at 0x7FDAF93D4F50>}))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# take a look at the available signatures for this particular model\n",
        "model.signatures.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfc9ax9hhCR6"
      },
      "source": [
        "Please choose the 'default' signature for your object detector.\n",
        "- For object detection models, its 'default' signature will accept a batch of image tensors and output a dictionary describing the objects detected, which is what you'll want here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pzwR5zE_hCR7"
      },
      "outputs": [],
      "source": [
        "detector = model.signatures['default']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvb-3r3thCR7"
      },
      "source": [
        "### download_and_resize_image\n",
        "\n",
        "This function downloads an image specified by a given \"url\", pre-processes it, and then saves it to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ucsxak_qhCR7"
      },
      "outputs": [],
      "source": [
        "def download_and_resize_image(url, new_width=256, new_height=256):\n",
        "    '''\n",
        "    Fetches an image online, resizes it and saves it locally.\n",
        "\n",
        "    Args:\n",
        "        url (string) -- link to the image\n",
        "        new_width (int) -- size in pixels used for resizing the width of the image\n",
        "        new_height (int) -- size in pixels used for resizing the length of the image\n",
        "\n",
        "    Returns:\n",
        "        (string) -- path to the saved image\n",
        "    '''\n",
        "\n",
        "\n",
        "    # create a temporary file ending with \".jpg\"\n",
        "    _, filename = tempfile.mkstemp(suffix=\".jpg\")\n",
        "\n",
        "    # opens the given URL\n",
        "    response = urlopen(url)\n",
        "\n",
        "    # reads the image fetched from the URL\n",
        "    image_data = response.read()\n",
        "\n",
        "    # puts the image data in memory buffer\n",
        "    image_data = BytesIO(image_data)\n",
        "\n",
        "    # opens the image\n",
        "    pil_image = Image.open(image_data)\n",
        "\n",
        "    # resizes the image. will crop if aspect ratio is different.\n",
        "    pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.Resampling.LANCZOS)\n",
        "\n",
        "    # converts to the RGB colorspace\n",
        "    pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "\n",
        "    # saves the image to the temporary file created earlier\n",
        "    pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n",
        "\n",
        "    print(\"Image downloaded to %s.\" % filename)\n",
        "\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7qodEJHhCR7"
      },
      "source": [
        "### Download and preprocess an image\n",
        "\n",
        "Now, using `download_and_resize_image` you can get a sample image online and save it locally.\n",
        "- We've provided a URL for you, but feel free to choose another image to run through the object detector.\n",
        "- You can use the original width and height of the image but feel free to modify it and see what results you get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xHTDalVrhCR7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image downloaded to /tmp/tmpmsw2iya1.jpg.\n"
          ]
        }
      ],
      "source": [
        "# You can choose a different URL that points to an image of your choice\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/f/fb/20130807_dublin014.JPG\"\n",
        "\n",
        "# download the image and use the original height and width\n",
        "downloaded_image_path = download_and_resize_image(image_url, 3872, 2592)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVNXUKMIhCR7"
      },
      "source": [
        "### run_detector\n",
        "\n",
        "This function will take in the object detection model `detector` and the path to a sample image, then use this model to detect objects and display its predicted class categories and detection boxes.\n",
        "- run_detector uses `load_image` to convert the image into a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wkkiQzKlhCR7"
      },
      "outputs": [],
      "source": [
        "def load_img(path):\n",
        "    '''\n",
        "    Loads a JPEG image and converts it to a tensor.\n",
        "\n",
        "    Args:\n",
        "        path (string) -- path to a locally saved JPEG image\n",
        "\n",
        "    Returns:\n",
        "        (tensor) -- an image tensor\n",
        "    '''\n",
        "\n",
        "    # read the file\n",
        "    img = tf.io.read_file(path)\n",
        "\n",
        "    # convert to a tensor\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def run_detector(detector, path):\n",
        "    '''\n",
        "    Runs inference on a local file using an object detection model.\n",
        "\n",
        "    Args:\n",
        "        detector (model) -- an object detection model loaded from TF Hub\n",
        "        path (string) -- path to an image saved locally\n",
        "    '''\n",
        "\n",
        "    # load an image tensor from a local file path\n",
        "    img = load_img(path)\n",
        "\n",
        "    # add a batch dimension in front of the tensor\n",
        "    converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
        "\n",
        "    # run inference using the model\n",
        "    result = detector(converted_img)\n",
        "\n",
        "    # save the results in a dictionary\n",
        "    result = {key:value.numpy() for key,value in result.items()}\n",
        "\n",
        "    # print results\n",
        "    print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n",
        "\n",
        "    print(result[\"detection_scores\"])\n",
        "    print(result[\"detection_class_entities\"])\n",
        "    print(result[\"detection_boxes\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSEeJSkxhCR7"
      },
      "source": [
        "### Run inference on the image\n",
        "\n",
        "You can run your detector by calling the `run_detector` function. This will print the number of objects found followed by three lists:\n",
        "\n",
        "* The detection scores of each object found (i.e. how confident the model is),\n",
        "* The classes of each object found,\n",
        "* The bounding boxes of each object\n",
        "\n",
        "You will see how to overlay this information on the original image in the next sections and in this week's assignment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "csanHvDIz4_t"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1738776628.311519    2655 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: -2484 } dim { size: -2485 } dim { size: -2486 } dim { size: 1088 } } } inputs { dtype: DT_FLOAT shape { dim { size: -105 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -105 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 17 } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 3090\" frequency: 1725 num_cores: 82 environment { key: \"architecture\" value: \"8.6\" } environment { key: \"cuda\" value: \"12050\" } environment { key: \"cudnn\" value: \"90300\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 6291456 shared_memory_size_per_multiprocessor: 102400 memory_size: 22827499520 bandwidth: 936096000 } outputs { dtype: DT_FLOAT shape { dim { size: -105 } dim { size: 17 } dim { size: 17 } dim { size: 1088 } } }\n",
            "I0000 00:00:1738776641.136579    3101 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 100 objects.\n",
            "[0.6543897  0.61137575 0.60415053 0.5930278  0.592491   0.5806505\n",
            " 0.55141443 0.49461365 0.4748336  0.4733283  0.4399835  0.41503626\n",
            " 0.40646893 0.39794672 0.39752257 0.37620464 0.37347975 0.36591738\n",
            " 0.35315752 0.3332758  0.30426687 0.27330685 0.2682141  0.25856954\n",
            " 0.25303397 0.24616821 0.23449393 0.20373291 0.18234532 0.18043084\n",
            " 0.17562005 0.16425191 0.15873097 0.15663753 0.15482292 0.15455694\n",
            " 0.14944755 0.13333549 0.12916057 0.12657215 0.12061741 0.11768289\n",
            " 0.11351996 0.11110626 0.11087005 0.10905053 0.10599481 0.08940269\n",
            " 0.08593106 0.08286179 0.08106495 0.07791033 0.07748125 0.07643165\n",
            " 0.07543032 0.07461057 0.07425645 0.07204162 0.07177822 0.0710707\n",
            " 0.07026292 0.06814579 0.06320512 0.06296485 0.06280147 0.06212726\n",
            " 0.05878795 0.05807635 0.0579273  0.05778307 0.05452029 0.05270756\n",
            " 0.05125809 0.04849778 0.04717498 0.04668472 0.04482817 0.04408841\n",
            " 0.04349162 0.04109617 0.04107363 0.03981362 0.03933355 0.0391546\n",
            " 0.03889631 0.03876756 0.03749823 0.03598769 0.03359969 0.03359662\n",
            " 0.03265046 0.03257643 0.03210784 0.02979373 0.02870927 0.02866915\n",
            " 0.02817893 0.02791565 0.02738586 0.02677402]\n",
            "[b'Person' b'Person' b'Person' b'Footwear' b'Person' b'Person' b'Building'\n",
            " b'Bicycle' b'Building' b'Window' b'Person' b'Bicycle' b'Wheel'\n",
            " b'Building' b'Building' b'Building' b'Person' b'Wheel' b'Window'\n",
            " b'Window' b'Building' b'Person' b'Van' b'Person' b'Bicycle wheel'\n",
            " b'Person' b'Window' b'Window' b'Building' b'Window' b'Window' b'Man'\n",
            " b'Person' b'Woman' b'Person' b'Clothing' b'Bicycle wheel' b'Window'\n",
            " b'Person' b'Window' b'Land vehicle' b'Land vehicle' b'Clothing' b'Window'\n",
            " b'Bicycle' b'Land vehicle' b'House' b'House' b'Man' b'Window' b'Clothing'\n",
            " b'Window' b'Footwear' b'Person' b'Man' b'Man' b'House' b'Building'\n",
            " b'Person' b'Clothing' b'Window' b'Person' b'Man' b'Furniture' b'Person'\n",
            " b'Jeans' b'Person' b'Person' b'Person' b'Land vehicle' b'Window' b'House'\n",
            " b'Woman' b'Man' b'Window' b'Person' b'Person' b'Clothing' b'Man'\n",
            " b'Window' b'Man' b'Car' b'Person' b'Man' b'Car' b'Chair' b'House'\n",
            " b'Window' b'Clothing' b'Tire' b'Window' b'Clothing' b'Land vehicle'\n",
            " b'Window' b'Man' b'Window' b'Van' b'Bus' b'Clothing' b'Car']\n",
            "[[5.12799561e-01 5.29271603e-01 6.01654530e-01 5.52096009e-01]\n",
            " [5.19733965e-01 6.01500750e-01 6.46120250e-01 6.34679496e-01]\n",
            " [5.05739927e-01 5.00443101e-01 6.01352215e-01 5.23089111e-01]\n",
            " [8.15194011e-01 9.56115127e-01 8.42701256e-01 9.87139106e-01]\n",
            " [4.86307800e-01 4.12774354e-01 6.78553104e-01 4.59912300e-01]\n",
            " [4.95474726e-01 9.23534751e-01 8.35634232e-01 9.99057055e-01]\n",
            " [1.11138411e-02 1.19252214e-02 7.39687443e-01 4.24988627e-01]\n",
            " [5.77802300e-01 3.66461456e-01 7.12810457e-01 4.83328134e-01]\n",
            " [7.75144696e-02 4.13050294e-01 5.79469800e-01 5.60328424e-01]\n",
            " [0.00000000e+00 1.19295739e-01 2.23891601e-01 1.83948964e-01]\n",
            " [5.14068902e-01 7.48100400e-01 5.91965318e-01 7.66567588e-01]\n",
            " [5.70760369e-01 3.61811608e-01 7.07344711e-01 4.29659635e-01]\n",
            " [6.32099211e-01 3.59871089e-01 7.03833044e-01 4.11814332e-01]\n",
            " [1.58900712e-02 6.84987724e-01 5.59408784e-01 8.11156809e-01]\n",
            " [0.00000000e+00 7.97082841e-01 6.73752248e-01 1.00000000e+00]\n",
            " [0.00000000e+00 2.17011124e-01 6.50898457e-01 4.32018846e-01]\n",
            " [5.00384510e-01 3.77002895e-01 6.33355379e-01 4.14522499e-01]\n",
            " [6.40320539e-01 4.45017099e-01 7.03031659e-01 4.83458668e-01]\n",
            " [1.95103337e-03 0.00000000e+00 1.39327928e-01 2.62863729e-02]\n",
            " [2.55017611e-03 9.66616929e-01 1.53758720e-01 1.00000000e+00]\n",
            " [1.41647342e-03 1.39732030e-03 7.64877379e-01 2.69356072e-01]\n",
            " [5.04891753e-01 3.60776782e-01 6.37643814e-01 3.85494739e-01]\n",
            " [4.83378619e-01 6.19501352e-01 5.62650979e-01 6.61570370e-01]\n",
            " [4.98194784e-01 3.64589721e-01 6.61132514e-01 4.04872984e-01]\n",
            " [6.31232440e-01 3.60325158e-01 7.04145253e-01 4.11496669e-01]\n",
            " [5.21807134e-01 5.77687562e-01 5.87610722e-01 6.00717425e-01]\n",
            " [2.19607070e-01 3.48742038e-01 3.38253170e-01 3.77069354e-01]\n",
            " [1.24819085e-01 2.50919074e-01 2.79908389e-01 2.81630158e-01]\n",
            " [2.57309079e-01 5.67498088e-01 5.30902088e-01 6.87873483e-01]\n",
            " [4.21755500e-02 8.74764025e-01 2.52856791e-01 9.13048744e-01]\n",
            " [1.56395227e-01 4.43363339e-01 2.22232923e-01 4.75781858e-01]\n",
            " [5.01998007e-01 9.21466410e-01 8.36351871e-01 1.00000000e+00]\n",
            " [5.23669064e-01 5.70343792e-01 5.84505081e-01 5.91616333e-01]\n",
            " [5.19167006e-01 5.99957407e-01 6.46318376e-01 6.34094179e-01]\n",
            " [5.13156116e-01 6.79228127e-01 5.50976932e-01 6.92547679e-01]\n",
            " [5.24352133e-01 9.24947083e-01 8.10506284e-01 9.97979224e-01]\n",
            " [6.38060391e-01 4.42799509e-01 7.01728046e-01 4.84129190e-01]\n",
            " [3.40994596e-02 3.55654359e-01 1.62304416e-01 3.74909580e-01]\n",
            " [4.88099694e-01 4.53371912e-01 6.22235000e-01 4.79666978e-01]\n",
            " [9.74852243e-04 3.07709694e-01 1.06501587e-01 3.32069486e-01]\n",
            " [4.82970893e-01 6.19806528e-01 5.64780593e-01 6.60650194e-01]\n",
            " [5.82375169e-01 3.64933074e-01 7.13888943e-01 4.84671026e-01]\n",
            " [5.23800671e-01 7.49292791e-01 5.85467219e-01 7.65311837e-01]\n",
            " [3.51467669e-01 9.74870861e-01 5.53047776e-01 9.98888612e-01]\n",
            " [6.09072983e-01 4.26830143e-01 7.05197573e-01 4.87112582e-01]\n",
            " [5.69233775e-01 3.59788477e-01 7.08571136e-01 4.28429395e-01]\n",
            " [0.00000000e+00 8.11159968e-01 6.93578243e-01 9.93240237e-01]\n",
            " [1.03982547e-02 2.29835864e-02 7.27232516e-01 4.22362655e-01]\n",
            " [4.84612942e-01 4.10707772e-01 6.94722533e-01 4.63141322e-01]\n",
            " [8.11469182e-02 3.84771526e-01 2.07954198e-01 4.11754876e-01]\n",
            " [5.38567841e-01 6.03576541e-01 6.34744048e-01 6.34472847e-01]\n",
            " [0.00000000e+00 1.24021387e-02 1.40323386e-01 2.47345064e-02]\n",
            " [6.29772782e-01 6.14873290e-01 6.44906759e-01 6.25331581e-01]\n",
            " [5.02840936e-01 3.82402599e-01 5.96048117e-01 4.12723869e-01]\n",
            " [5.14679670e-01 7.47874558e-01 5.91949463e-01 7.66781151e-01]\n",
            " [5.06423891e-01 5.00404835e-01 6.00720406e-01 5.23319662e-01]\n",
            " [0.00000000e+00 2.11072519e-01 6.50793910e-01 4.34388399e-01]\n",
            " [0.00000000e+00 7.06335306e-01 6.17184401e-01 8.66043448e-01]\n",
            " [4.89300758e-01 4.54279780e-01 5.72645485e-01 4.76397395e-01]\n",
            " [5.09200156e-01 4.16266561e-01 6.69002116e-01 4.59580064e-01]\n",
            " [4.67978790e-03 8.03110242e-01 1.59552649e-01 8.40365350e-01]\n",
            " [5.26165247e-01 5.68374574e-01 5.79431951e-01 5.82803249e-01]\n",
            " [5.02825856e-01 3.73994082e-01 6.47136271e-01 4.12980914e-01]\n",
            " [5.74184358e-01 2.67241955e-01 6.57759309e-01 3.20313871e-01]\n",
            " [4.85912532e-01 4.44438070e-01 6.24697745e-01 4.73513067e-01]\n",
            " [6.71993732e-01 9.40320194e-01 8.21178019e-01 9.89215076e-01]\n",
            " [5.24100244e-01 5.61539531e-01 5.78335762e-01 5.80483377e-01]\n",
            " [5.17587960e-01 7.57212400e-01 5.88311017e-01 7.71542966e-01]\n",
            " [5.23333132e-01 5.57809651e-01 5.79024076e-01 5.73545218e-01]\n",
            " [6.12351358e-01 4.27394211e-01 7.06093013e-01 4.88303274e-01]\n",
            " [0.00000000e+00 2.44243145e-01 6.08964041e-02 2.93773770e-01]\n",
            " [1.55006154e-02 1.91993383e-03 7.45189607e-01 2.59332061e-01]\n",
            " [4.93278623e-01 9.23959911e-01 8.36901486e-01 9.97704268e-01]\n",
            " [5.05275965e-01 3.60157996e-01 6.43361092e-01 3.91440332e-01]\n",
            " [8.42804275e-03 2.42120624e-01 4.97423969e-02 2.83139408e-01]\n",
            " [5.22107065e-01 5.36090255e-01 5.97667754e-01 5.53135335e-01]\n",
            " [5.13131976e-01 5.23811817e-01 6.00541234e-01 5.42966485e-01]\n",
            " [5.18305600e-01 5.03451765e-01 5.97546697e-01 5.22744715e-01]\n",
            " [5.20449519e-01 6.00926042e-01 6.45989895e-01 6.34363592e-01]\n",
            " [4.29730028e-01 8.28745008e-01 5.90024650e-01 8.64377141e-01]\n",
            " [5.13168812e-01 6.79253995e-01 5.50482035e-01 6.92442119e-01]\n",
            " [5.26587307e-01 6.27183139e-01 5.63288271e-01 6.53777719e-01]\n",
            " [5.04783034e-01 3.89389366e-01 6.15238845e-01 4.19952035e-01]\n",
            " [5.01316488e-01 3.64211023e-01 6.59732342e-01 4.03692871e-01]\n",
            " [5.15086114e-01 6.24091983e-01 5.63828826e-01 6.58030272e-01]\n",
            " [5.73123336e-01 2.66729951e-01 6.66216671e-01 3.18647087e-01]\n",
            " [8.32108855e-02 4.07569915e-01 5.84339499e-01 5.58325171e-01]\n",
            " [2.88206577e-01 4.68888466e-04 4.14292276e-01 3.67029570e-02]\n",
            " [4.97162640e-01 4.55214292e-01 5.84271908e-01 4.77872550e-01]\n",
            " [6.27136052e-01 3.61000568e-01 7.05955505e-01 4.09778208e-01]\n",
            " [1.17210839e-02 3.08074832e-01 9.73203629e-02 3.25075835e-01]\n",
            " [5.15901685e-01 3.80088598e-01 5.96969903e-01 4.11772162e-01]\n",
            " [5.12413383e-01 6.23646975e-01 5.62431812e-01 6.57678545e-01]\n",
            " [4.00786310e-01 8.84969294e-01 5.81628919e-01 9.39135909e-01]\n",
            " [5.13907850e-01 5.29503047e-01 6.02048099e-01 5.52377641e-01]\n",
            " [0.00000000e+00 9.94701404e-03 1.36246920e-01 3.15967873e-02]\n",
            " [5.10678470e-01 6.24041736e-01 5.63403666e-01 6.58179998e-01]\n",
            " [4.80376482e-01 6.20339096e-01 5.65285146e-01 6.60120368e-01]\n",
            " [5.38418591e-01 9.28022683e-01 7.13667750e-01 9.99452770e-01]\n",
            " [4.86334354e-01 6.20263100e-01 5.63521743e-01 6.60216510e-01]]\n"
          ]
        }
      ],
      "source": [
        "# runs the object detection model and prints information about the objects found\n",
        "run_detector(detector, downloaded_image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.-1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
